## 1、机器学习常见面试题

   网址：[https://github.com/DarLiner/Algorithm_Interview_Notes-Chinese/tree/master/A-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0](https://github.com/DarLiner/Algorithm_Interview_Notes-Chinese/tree/master/A-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0)

## 2、机器学习常见面试题整理：

    主要涉及内容：有监督学习和无监督学习的区别、正则化、过拟合、生成模型和判别模型、线性分类器与非线性分类器的区别以及优劣、L1和L2正则的区别
    
    网址：[http://kubicode.me/2015/08/16/Machine%20Learning/Common-Interview/](http://kubicode.me/2015/08/16/Machine%20Learning/Common-Interview/)
    （也可以看看，跟上面重复角度）https://blog.csdn.net/timcompp/article/details/62237986

## 3、机器学习算法常见面试题目总结

     主要内容：逻辑回归的模型、原理；Logistic回归分类器；过拟合怎么解决，L1和L2正则化有什么区别；怎么理解损失函数，SVM的损失函数是什么，写出公式；偏差和方差是什么，高偏差和高方差说明了什么；激活函数；

网址：[https://blog.csdn.net/qq_33690342/article/details/82631126](https://blog.csdn.net/qq_33690342/article/details/82631126)

## 4、史上最全！41道 Machine Learning 高频面试题都在这里了。

        主要内容：什么是偏差（bias）、方差（variable）之间的均衡；监督学习和非监督学习有什么不同；KNN和 k-means 聚类由什么不同；什么是贝叶斯定理？它在机器学习环境中如何有用?；第一类误差和第二类误差有什么区别；生成模型与判别模型有什么区别；什么是F1数，怎么使用它；如何处理一个不平衡的数据集；

      网址：[https://www.dataapplab.com/machine-learning-interview-questions/](https://www.dataapplab.com/machine-learning-interview-questions/)

## 5、机器学习与深度学习常见面试题（上）

      主要内容：无监督学习中存在过拟合吗；请列举几种常见的激活函数。激活函数有什么作用；如何解决不平衡数据集的分类问题；

      网址： [https://zhuanlan.zhihu.com/p/45091568](https://zhuanlan.zhihu.com/p/45091568)

6、 机器学习与深度学习常见面试题（下）（这个可以大致看看）

     主要内容：为什么随机森林能降低方差；Logistic回归为什么用交叉熵而不用欧氏距离做损失函数；什么是反卷积；神经网络是生成模型还是判别模型；典型人脸识别系统的识别流程

网址：[https://www.itcodemonkey.com/article/10699.html](https://www.itcodemonkey.com/article/10699.html)

## 7、KNN和Kmeans对比比较：

    主要内容：算法流程和对比

网址：[https://blog.csdn.net/sinat_35512245/article/details/55051306](https://blog.csdn.net/sinat_35512245/article/details/55051306)

可参考：[https://liyonghui160com.iteye.com/blog/2085961](https://liyonghui160com.iteye.com/blog/2085961)

k-means：首先从n个数据对象任意选择k个对象作为初始聚类中心；而对于所剩下其它对象，则根据它们与这些聚类中心的相似度（距离），分别将它们分配给与其最相似的（聚类中心所代表的）聚类；然后再计算每个所获新聚类的聚类中心（该聚类中所有对象的均值）；不断重复这一过程直到标准测度函数开始收敛为止。一般都采用均方差作为标准测度函数.k个聚类具有以下特点：各聚类本身尽可能的紧凑，而各聚类之间尽可能的分开。

K最近邻(k-NearestNeighbor，KNN)分类算法：该方法的思路是：如果一个样本在特征空间中的**k个最相似**(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。KNN算法中，所选择的邻居都是已经正确分类的对象。该方法在定类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。KNN方法虽然从原理上也依赖于极限定理，但在类别决策时，只与极少量的相邻样本有关。由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合。KNN算法不仅可以用于分类，还可以用于回归。通过找出一个样本的k个最近邻居，将这些邻居的属性的平均值赋给该样本，就可以得到该样本的属性。更有用的方法是将不同距离的邻居对该样本产生的影响给予不同的权值(weight)，如权值与距离成正比。

8、主成成分分析：

   主要内容：算法流程

网址：[https://blog.csdn.net/ckzhb/article/details/75275008](https://blog.csdn.net/ckzhb/article/details/75275008)

（也可参考）[https://www.kancloud.cn/baozou/stanfordml/562717](https://www.kancloud.cn/baozou/stanfordml/562717)

设有m条n维数据。

1）将原始数据按列组成n行m列矩阵X。

2）将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值。

即数据预处理，更准确的做法为：

3）求出协方差矩阵Cov。

4）求出协方差矩阵的特征值及对应的特征向量或者利用奇异值分解的方法。

5）将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P。

6）Y=PX即为降维到k维后的数据

---



## 9、各种算法优缺点

| 算法    | 优点                                                                                                                                                                                                                                                                                                                                                                     | 缺点                                                                                                                                                                                                                                                                                                                                                                                                    |
| ----- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| KNN   | 1.简单、有效。2.重新训练的代价较低（类别体系的变化和训练集的变化，在Web环境和电子商务应用中是很常见的）3.计算时间和空间线性于训练集的规模（在一些场合不算太大）4.由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合。5.该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量较小的类域采用这种算法比较容易产生误分。https://blog.csdn.net/u011239443/article/details/76360294                                                                                    | 1.KNN算法是懒散学习方法（lazy learning,基本上不学习），一些积极学习的算法要快很多。2.类别评分不是规格化的（不像概率评分）。3.输出的可解释性不强，例如决策树的可解释性较强。4.该算法在分类时有个主要的不足是，当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。该算法只计算“最近的”邻居样本，某一类的样本数量很大，那么或者这类样本并不接近目标样本，或者这类样本很靠近目标样本。无论怎样，数量并不能影响运行结果。可以采用权值的方法（和该样本距离小的邻居权值大）来改进。5.计算量较大。目前常用的解决方法是事先对已知样本点进行剪辑，事先去除对分类作用不大的样本。
https://blog.csdn.net/u011239443/article/details/76360294 |
| 朴素贝叶斯 | 1.朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率。2.NBC模型所需估计的参数很少，对缺失数据不太敏感，算法也比较简单，常用于文本分类。3.对小规模的数据表现很好，能个处理多分类任务，适合增量式训练，尤其是数据量超出内存时，我们可以一批批的去增量训练。                                                                                                                                                                                                                           | 1.理论上，NBC模型与其他分类方法相比具有最小的误差率。但是实际上并非总是如此，这是因为NBC模型假设属性之间相互独立，这个假设在实际应用中往往是不成立的（可以考虑用聚类算法先将相关性较大的属性聚类），这给NBC模型的正确分类带来了一定影响。在属性个数比较多或者属性之间相关性较大时，NBC模型的分类效率比不上决策树模型。而在属性相关性较小时，NBC模型的性能最为良好。2.需要知道先验概率，且先验概率很多时候取决于假设，假设的模型可以有很多种，因此在某些时候会由于假设的先验模型的原因导致预测效果不佳。3.由于我们是通过先验和数据来决定后验的概率从而决定分类，所以分类决策存在一定的错误率。4.对输入数据的表达形式很敏感。https://blog.csdn.net/u011239443/article/details/76360294                    |
| 决策树   | 1.决策树易于理解和解释.人们在通过解释后都有能力去理解决策树所表达的意义。2.对于决策树，数据的准备往往是简单或者是不必要的.其他的技术往往要求先把数据一般化，比如去掉多余的或者空白的属性。3.能够同时处理数据型和常规型属性。其他的技术往往要求数据属性的单一。4.决策树是一个白盒模型。如果给定一个观察的模型，那么根据所产生的决策树很容易推出相应的逻辑表达式。5.易于通过静态测试来对模型进行评测。表示有可能测量该模型的可信度。6.在相对短的时间内能够对大型数据源做出可行且效果良好的结果。7.可以对有许多属性的数据集构造决策树。8.决策树可很好地扩展到大型数据库中，同时它的大小独立于数据库的大小。https://blog.csdn.net/u011239443/article/details/76360294 | 1.对于那些各类别样本数量不一致的数据，在决策树当中,信息增益的结果偏向于那些具有更多数值的特征。2.决策树处理缺失数据时的困难。3.过度拟合问题的出现。4. 忽略数据集中属性之间的相关性。                                                                                                                                                                                                                                                                                                       |
| 支持向量机 | 1.可以解决小样本情况下的机器学习问题。2.可以提高泛化性能。3.可以解决高维问题。4.可以解决非线性问题。5.可以避免神经网络结构选择和局部极小点问题。                                                                                                                                                                                                                                                                                          | 1.对缺失数据敏感。2.对非线性问题没有通用解决方案，必须谨慎选择Kernel function来处理。                                                                                                                                                                                                                                                                                                                                                  |
| 神经网络  | 1.分类的准确度高2.并行分布处理能力强,分布存储及学习能力强3.对噪声神经有较强的鲁棒性和容错能力4.能充分逼近复杂的非线性关系5.具备联想记忆的功能等                                                                                                                                                                                                                                                                                          | 1.神经网络需要大量的参数，如网络拓扑结构、权值和阈值的初始值2.不能观察之间的学习过程，输出结果难以解释，会影响到结果的可信度和可接受程度3.学习时间过长,甚至可能达不到学习的目的。                                                                                                                                                                                                                                                                                                          |
