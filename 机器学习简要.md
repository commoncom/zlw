## 1、有监督学习和无监督学习的区别

- 有监督学习：对具有标记的训练样本进行学习，以尽可能对训练样本集外的数据进行分类预测。（LR,SVM,BP,RF,GBRT）
- 无监督学习：对未标记的样本进行训练学习，比发现这些样本中的结构知识。(KMeans,DL)

## 2、正则化

正则化是针对过拟合而提出的，以为在求解模型最优的是一般优化最小的经验风险，现在在该经验风险上加入模型复杂度这一项（正则化项是模型参数向量的范数），并使用一个rate比率来权衡模型复杂度与以往经验风险的权重，如果模型复杂度越高，结构化的经验风险会越大，现在的目标就变为了结构经验风险的最优化，可以防止模型训练过度复杂，有效的降低过拟合的风险。

> 奥卡姆剃刀原理，能够很好的解释已知数据并且十分简单才是最好的模型。

## 3、过拟合

如果一味的去提高训练数据的预测能力，所选模型的复杂度往往会很高，这种现象称为过拟合。所表现的就是模型训练时候的误差很小，但在测试的时候误差很大。

### 产生的原因

1. 因为参数太多，会导致我们的模型复杂度上升，容易过拟合
2. 权值学习迭代次数足够多(Overtraining),拟合了训练数据中的噪声和训练样例中没有代表性的特征.

### 解决方法

1. 交叉验证法
2. 减少特征
3. 正则化
4. 权值衰减
5. 验证数据

## 4、泛化能力

泛化能力是指模型对未知数据的预测能力

## 5、生成模型和判别模型

1. 生成模型：由数据学习联合概率分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型：P(Y|X)= P(X,Y)/ P(X)。（朴素贝叶斯）  
   
   生成模型可以还原联合概率分布p(X,Y)，并且有较快的学习收敛速度，还可以用于隐变量的学习
2. 判别模型：由数据直接学习决策函数Y=f(X)或者条件概率分布P(Y|X)作为预测的模型，即判别模型。（k近邻、决策树）  
   
   直接面对预测，往往准确率较高，直接对数据在各种程度上的抽象，所以可以简化模型

## 6、线性分类器与非线性分类器的区别以及优劣

如果模型是参数的线性函数，并且存在线性分类面，那么就是线性分类器，否则不是。  
常见的线性分类器有：LR,贝叶斯分类，单层感知机、线性回归  
常见的非线性分类器：决策树、RF、GBDT、多层感知机

> SVM两种都有(看线性核还是高斯核)

- 线性分类器速度快、编程方便，但是可能拟合效果不会很好
- 非线性分类器编程复杂，但是效果拟合能力强

### 7、特征比数据量还大时，选择什么样的分类器？

线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分

### 8、对于维度很高的特征，你是选择线性还是非线性分类器？

理由同上

### 9、对于维度极低的特征，你是选择线性还是非线性分类器？

非线性分类器，因为低维空间可能很多特征都跑到一起了，导致线性不可分

## 10、ill-condition病态问题

训练完的模型测试样本稍作修改就会得到差别很大的结果，就是病态问题（这简直是不能用啊）

## 11、L1和L2正则的区别，如何选择L1和L2正则

> 他们都是可以防止过拟合，降低模型复杂度

- L1是在loss function后面加上 模型参数的1范数（也就是|xi|）

- L2是在loss function后面加上 模型参数的2范数（也就是sigma(xi^2)），注意L2范数的定义是sqrt(sigma(xi^2))，在正则项上没有添加sqrt根号是为了更加容易优化

- L1 会产生稀疏的特征

- L2 会产生更多地特征但是都会接近于0

L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。L1在特征选择时候非常有用，而L2就只是一种规则化而已。

## 12、特征向量的归一化方法

1. 线性函数转换，表达式如下：y=(x-MinValue)/(MaxValue-MinValue)
2. 对数函数转换，表达式如下：y=log10 (x)
3. 反余切函数转换 ，表达式如下：y=arctan(x)*2/PI
4. 减去均值，乘以方差：y=(x-means)/ variance

## 13、特征向量的异常值处理

1. 用均值或者其他统计量代替

## 14、KMeans初始类簇中心点的选取

### 选择批次距离尽可能远的K个点

```
首先随机选取一个点作为初始点，然后选择距离与该点最远的那个点作为中心点，再选择距离与前两个点最远的店作为第三个中心店，以此类推，直至选取大k个
```

### 选用层次聚类或者Canopy算法进行初始聚类

## 15、**逻辑回归的模型、原理**

      假设限制有一些数据点，我们利用一条直线对这些点进行拟合（该线称为最佳拟合直线），这个拟合过程就成为回归。logistic回归进行分类的思想是：根据现有数据对分类边界线建立回归公式，以此进行分类。训练分类器时的做法就是寻找最佳拟合参数，使用的是最优化算法。
    
     Logistic回归分类器，我们在每一个特征上都乘以一个回归系数，然后把所有的结果值相加，将这个总和代入Sigmoid函数中，进而得到一个范围在0~1之间的数值。任何大于0.5的数据都被分入1类，小于0.5的即被归入0类。所以，logistic回归也可以被看成是一种概率估计。

## 16、随机梯度下降算法的原理和具体迭代函数

    梯度： 　在微积分里面，对多元函数的参数求∂偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。比如函数f(x,y), 分别对x,y求偏导数，求得的梯度向量就是(∂f/∂x, ∂f/∂y)T,简称grad f(x,y)或者▽f(x,y)。对于在点(x0,y0)的具体梯度向量就是(∂f/∂x0, ∂f/∂y0)T.或者▽f(x0,y0)，如果是3个参数的向量梯度，就是(∂f/∂x, ∂f/∂y，∂f/∂z)T,以此类推。

　　　　那么这个梯度向量求出来有什么意义呢？他的意义从几何意义上讲，就是函数变化增加最快的地方。具体来说，对于函数f(x,y),在点(x0,y0)，沿着梯度向量的方向就是(∂f/∂x0, ∂f/∂y0)T的方向是f(x,y)增加最快的地方。或者说，沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向，也就是 -(∂f/∂x0, ∂f/∂y0)T的方向，梯度减少最快，也就是更加容易找到函数的最小值。

     流程：[https://www.cnblogs.com/pinard/p/5970503.html](https://www.cnblogs.com/pinard/p/5970503.html)

17、怎么理解损失函数，SVM的损失函数是什么，写出公式**

     损失函数是用来测度模型输出值和真实的因变量值之间差异的函数，模型输出值p和和真实值y之间的差异一般称为残差或者误差，但是这个值并不能直接用来衡量模型的质量，当一个模型完美的时候，其误差为0，而当一个模型不够完美时，其误差不论是负值还是正值，都偏离0，因此衡量模型质量的误差偏离0的相对值，即误差函数的值越接近于0，模型的性能越好，反之模型的性能越差。常用的损失函数如下：

均方差（MSE）误差函数：   这种损失函数常用在实数值域连续变量的回归问题上，并且对于残差较大的情况给与更多的权重。
      平均绝对差(MAE)：这种损失函数也常用在实数值域连续变量的回归问题上，在时间序列预测问题中也常用。在这个误差函数中每个误差点对总体误差的贡献与其误差绝对值成线性比例关系，而上面的MSE没有此特性。
      交叉熵损失函数(Cross-Entropy):这种损失函数也叫对数损失函数，是针对分类模型的性能比较设计的，按照分类模型是二分类还是多分类的区别，可以分为二分类交叉熵和多分类交叉熵两种,交叉熵的数学表达式如下：

因此交叉熵可以解释为映射到最可能类别的概率的对数。因此，当预测值的分布和实际因变量的分布尽可能一致时，交叉熵最小。

SVM的损失函数：常用的是径向基函数（RBF）:
---------------------

![5d273d71173dc77102](https://i.loli.net/2019/07/11/5d273d71173dc77102.png)

## 18、无监督学习中存在过拟合吗？

存在。我们可以使用无监督学习的某些指标或人为地去评估模型性能，以此来判断是否过拟合。

## 19、KNN和Kmeans对比

![5d273eaf8b77b61836](https://i.loli.net/2019/07/11/5d273eaf8b77b61836.png)

## 20 、软间隔SVM与逻辑回归的区别

        逻辑回归通过输出预测概率后根据阈值进行判断类别，SVM则直接输出分割超平面，然后使用0/1函数对距离进行分类，不能直接输出概率值，如果需要SVM输出概率值则需要进行特殊处理，可以根据距离的大小进行归一化概率输出。
       逻辑回归可以使用多阈值然后进行多分类，SVM则需要进行推广。

      SVM在训练过程只需要支持向量的，依赖的训练样本数较小，而逻辑回归则是需要全部的训练样本数据，在训练时开销更大。

### 21、梯度消失产生的原因？

- 反向传播，前一层的梯度是由后一层梯度的乘积计算得到。假如每一层上的梯度小于1，越乘越小，到最前面的层就会梯度消失。

- S型神经元，在01附近梯度很小，而w和b的梯度是有S函数的梯度因式，从而导致梯度变小

## 22、业务场景：

问题一
数据格式： (用户特征，商品特征)，预测用户是否会购买该商品？

这里给出之前在导师公司做的一个解决方法： 
1.根据用户特征，按照一定规则给用户打上基础标签。 
2.根据基础标签，做聚类，得到K个聚类中心。 
3.各个聚类中心下的各个用户的所有（商品特征，是否购买）作为训练集，训练出K个二分类模型。 

4.测试数据根据其用户特征与各中心的距`离，将其归为最近的聚类重心C下。根据商品特征，使用对应的二分类模型，进行预测。`

## 23、什么是神经网络的梯度消失问题，为什么会有梯度消失问题？有什么办法能缓解梯度消失问题？

在反向传播算法计算每一层的误差项的时候，需要乘以本层激活函数的导数值，如果导数值接近于0，则多次乘积之后误差项会趋向于0，而参数的梯度值通过误差项计算，这会导致参数的梯度值接近于0，无法用梯度下降法来有效的更新参数的值。

改进激活函数，选用更不容易饱和的函数，如ReLU函数。

## 24、列举你所知道的神经网络中使用的损失函数

欧氏距离，交叉熵，对比损失，合页损失

30.对于多分类问题，为什么神经网络一般使用交叉熵而不用欧氏距离损失？

交叉熵在一般情况下更容易收敛到一个更好的解。

## 25、梯度下降法，为什么需要设置一个学习率？

使得迭代之后的值在上次值的邻域内，保证可以忽略泰勒展开中的二次及二次以上的项

## 26、在训练深度神经网络的过程中，遇到过哪些问题，怎么解决的？

不收敛，收敛太慢，泛化能力差。调整网络结构，调整样本，调整学习率，调整参数初始化策略

## 27、K均值算法中，初始类中心怎么确定

随机选择K个样本作为类中心，将样本随机划分成K个子集然后计算类中心
